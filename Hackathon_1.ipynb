{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Restaurant Turnover Prediction**"
      ],
      "metadata": {
        "id": "7VZN147KMaSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Business Context**\n",
        "\n",
        "The first thing any visitor to India will take in — probably while staring out the window in awe as their aeroplane descends is the sheer size of this country. It is densely populated and patchworked with distinct neighbourhoods, each with its own culinary identity. It would take several lifetimes to get to know all of the street stands, holes in the wall, neighbourhood favourites, and high-end destinations in this city.\n",
        "And for Indians dining out is and always will be a joyous occasion. Everyone has their own favourite restaurants in the city starting from the street food stall across the street to the 5-star restaurants in the heart of the city. Some are favourites because of the memory attached to it and some are favourites because of the fact that the place has a fantastic ambience. There are a lot of other factors as well which contribute to the likeness of the restaurants which in turn determines their popularity among the masses.\n",
        "\n",
        "If you look at this from the business perspective for a restaurant, more popularity may mean more visits to the joint increasing the annual turnover of the restaurants. For any restaurant to survive and do well, the annual turnover of the restaurants has to be substantial.\n",
        "\n",
        "This problem takes a shot at predicting the annual turnover of a set of restaurants across India based on a set of variables given in the data set. This includes the data related to the restaurant such as location, opening date, cuisine type, themes etc. This also includes data pooled from different sources such as social media popularity index, Zomato ratings, etc. Lastly, it also adds a different flavour to the problem by looking at the Customer survey data as well as ratings provided by mystery visitor data (audit done by a third party)."
      ],
      "metadata": {
        "id": "h8C5--IqMku8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Objective**\n",
        "The goal of this problem is to predict the Annual Turnover of a restaurant based on the variables provided in the data set."
      ],
      "metadata": {
        "id": "UxAbFqeBMq3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Metric to measure**\n",
        "\n",
        "The measure of accuracy will be RMSE (Root mean square error)\n",
        "\n",
        "The predicted Annual Turnover for each restaurant in the Test dataset will be compared with the actual Annual Turnover to calculate the RMSE value of the entire prediction. The lower the RMSE value, the better the model will be."
      ],
      "metadata": {
        "id": "rI-AI1jSM449"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOslMK7prL5R",
        "outputId": "82ddeaf4-6c6c-4acd-b064-a068a7ddf37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary packages\n",
        "!pip install optuna catboost xgboost scikit-learn category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQlkEVahrNF9",
        "outputId": "221f9ef9-9bcc-47ac-d169-0f1ae2112d65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna, catboost, category_encoders\n",
            "Successfully installed alembic-1.16.1 catboost-1.2.8 category_encoders-2.8.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#update xgboost\n",
        "!pip install --upgrade xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxIIpP72rX0C",
        "outputId": "bc974faa-373c-4480-81cb-84b6aff1be85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "Successfully installed xgboost-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import Counter\n",
        "import optuna\n",
        "# Import additional models\n",
        "from catboost import CatBoostRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor # Another alternative GBDT\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "import category_encoders as ce"
      ],
      "metadata": {
        "id": "JLjiwpN6ri2M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the datasets\n",
        "train_df_orig = pd.read_csv('/content/drive/MyDrive/UT AI ML PG program/Hackathon1/Day 1/Train_dataset.csv')\n",
        "test_df_orig = pd.read_csv('/content/drive/MyDrive/UT AI ML PG program/Hackathon1/Day 1/Test_dataset.csv')"
      ],
      "metadata": {
        "id": "28bZ6tGSrqEr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model building\n",
        "test_df_orig.rename(columns={'Endoresed By': 'Endorsed By'}, inplace=True)\n",
        "\n",
        "train_reg_nums = train_df_orig['Registration Number']\n",
        "test_reg_nums = test_df_orig['Registration Number']\n",
        "train_target = train_df_orig['Annual Turnover']\n",
        "train_df = train_df_orig.drop('Annual Turnover', axis=1)\n",
        "test_df = test_df_orig.copy()\n",
        "\n",
        "combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
        "train_len = len(train_df)\n",
        "\n",
        "# --- Feature Engineering & Preprocessing (Same as before) ---\n",
        "# 1. Opening Day of Restaurant\n",
        "combined_df['Opening Day of Restaurant'] = pd.to_datetime(combined_df['Opening Day of Restaurant'], format='%d-%m-%Y', errors='coerce')\n",
        "latest_date_in_data = combined_df['Opening Day of Restaurant'].max()\n",
        "reference_date = datetime(2014, 3, 1)\n",
        "if latest_date_in_data is not pd.NaT and latest_date_in_data > reference_date :\n",
        "    reference_date = latest_date_in_data + pd.Timedelta(days=1)\n",
        "\n",
        "combined_df['Restaurant_Age_Days'] = (reference_date - combined_df['Opening Day of Restaurant']).dt.days\n",
        "combined_df['Opening_Year'] = combined_df['Opening Day of Restaurant'].dt.year\n",
        "combined_df['Opening_Month'] = combined_df['Opening Day of Restaurant'].dt.month\n",
        "combined_df['Opening_DayOfWeek'] = combined_df['Opening Day of Restaurant'].dt.dayofweek\n",
        "\n",
        "date_cols_to_impute = ['Restaurant_Age_Days', 'Opening_Year', 'Opening_Month', 'Opening_DayOfWeek']\n",
        "for col in date_cols_to_impute:\n",
        "    median_val_train = combined_df.loc[:train_len-1, col].median()\n",
        "    combined_df[col].fillna(median_val_train, inplace=True)\n",
        "combined_df = combined_df.drop('Opening Day of Restaurant', axis=1)\n",
        "\n",
        "# 2. Numerical Columns\n",
        "numerical_cols_to_impute_median = ['Facebook Popularity Quotient', 'Instagram Popularity Quotient']\n",
        "for col in numerical_cols_to_impute_median:\n",
        "    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
        "    median_val_train = combined_df.loc[:train_len-1, col].median()\n",
        "    combined_df[col].fillna(median_val_train, inplace=True)\n",
        "\n",
        "rating_cols = ['Restaurant Zomato Rating', 'Order Wait Time', 'Staff Responsivness', 'Value for Money',\n",
        "               'Hygiene Rating', 'Food Rating', 'Overall Restaurant Rating', 'Live Music Rating',\n",
        "               'Comedy Gigs Rating', 'Value Deals Rating', 'Live Sports Rating', 'Ambience',\n",
        "               'Lively', 'Service', 'Comfortablility', 'Privacy']\n",
        "for col in rating_cols:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df[col].replace('NA', np.nan)\n",
        "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
        "        combined_df[col].fillna(-1, inplace=True)\n",
        "\n",
        "# 3. Categorical Columns\n",
        "# City\n",
        "combined_df['City'] = combined_df['City'].astype(str).str.lower().str.strip()\n",
        "combined_df['City'] = combined_df['City'].replace(['-1', 'nan', '', 'na'], 'unknown_city')\n",
        "city_counts_train = combined_df.loc[:train_len-1, 'City'].value_counts()\n",
        "# Increased threshold slightly for city grouping\n",
        "rare_cities_train = city_counts_train[city_counts_train < 15].index\n",
        "combined_df['City'] = combined_df['City'].apply(lambda x: 'other_city' if x in rare_cities_train and x != 'unknown_city' else x)\n",
        "\n",
        "# Cuisine\n",
        "combined_df['Cuisine'] = combined_df['Cuisine'].fillna('unknown').astype(str).str.lower()\n",
        "cuisine_list_series = combined_df['Cuisine'].apply(lambda x: [c.strip() for c in x.split(',') if c.strip()])\n",
        "combined_df['Num_Cuisines'] = cuisine_list_series.apply(len)\n",
        "flat_cuisine_list_train = [item for sublist in cuisine_list_series[:train_len] for item in sublist if item != 'unknown']\n",
        "cuisine_counts_train_dict = Counter(flat_cuisine_list_train)\n",
        "# Slightly more cuisines\n",
        "top_n_cuisines = [cuisine for cuisine, count in cuisine_counts_train_dict.most_common(30)]\n",
        "for cuisine_name in top_n_cuisines:\n",
        "    cleaned_name = re.sub(r'\\W+', '_', cuisine_name)\n",
        "    combined_df[f'Cuisine_{cleaned_name}'] = cuisine_list_series.apply(lambda x: 1 if cuisine_name in x else 0)\n",
        "combined_df = combined_df.drop('Cuisine', axis=1)\n",
        "\n",
        "# Restaurant Theme\n",
        "combined_df['Restaurant Theme'] = combined_df['Restaurant Theme'].fillna('Unknown_Theme').astype(str)\n",
        "theme_counts_train = combined_df.loc[:train_len-1, 'Restaurant Theme'].value_counts()\n",
        "# Slightly more aggressive grouping for themes\n",
        "rare_themes_train = theme_counts_train[theme_counts_train < 15].index\n",
        "combined_df['Restaurant Theme'] = combined_df['Restaurant Theme'].apply(lambda x: 'Other_Theme' if x in rare_themes_train and x != 'Unknown_Theme' else x)\n",
        "\n",
        "# Resturant Tier\n",
        "mode_tier_train = combined_df.loc[:train_len-1, 'Resturant Tier'].mode()\n",
        "fill_tier_value = mode_tier_train[0] if not mode_tier_train.empty else '2.0' # Default if mode is empty\n",
        "combined_df['Resturant Tier'] = combined_df['Resturant Tier'].fillna(fill_tier_value)\n",
        "combined_df['Resturant Tier'] = combined_df['Resturant Tier'].astype(str)\n",
        "\n",
        "\n",
        "cat_cols_simple_impute_mode = ['Restaurant Location', 'Endorsed By', 'Restaurant Type', 'Restaurant City Tier']\n",
        "for col in cat_cols_simple_impute_mode:\n",
        "    mode_val_train = combined_df.loc[:train_len-1, col].mode()\n",
        "    fill_value = mode_val_train[0] if not mode_val_train.empty else 'Unknown'\n",
        "    combined_df[col].fillna(fill_value, inplace=True)\n",
        "    combined_df[col] = combined_df[col].astype(str)\n",
        "\n",
        "binary_cols = ['Fire Audit', 'Liquor License Obtained', 'Situated in a Multi Complex', 'Dedicated Parking', 'Open Sitting Available']\n",
        "for col in binary_cols:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df[col].astype(int)\n",
        "combined_df = combined_df.drop('Registration Number', axis=1)\n",
        "\n",
        "categorical_features_for_ohe = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
        "if 'Resturant Tier' not in categorical_features_for_ohe: categorical_features_for_ohe.append('Resturant Tier')\n",
        "if 'Restaurant City Tier' not in categorical_features_for_ohe: categorical_features_for_ohe.append('Restaurant City Tier')\n",
        "categorical_features_for_ohe = [col for col in categorical_features_for_ohe if not col.startswith('Cuisine_')]\n",
        "combined_df = pd.get_dummies(combined_df, columns=[col for col in categorical_features_for_ohe if col in combined_df.columns], dummy_na=False)\n",
        "\n",
        "processed_train_df = combined_df.iloc[:train_len].copy()\n",
        "processed_test_df = combined_df.iloc[train_len:].copy()\n",
        "train_cols = set(processed_train_df.columns)\n",
        "test_cols = set(processed_test_df.columns)\n",
        "for col in train_cols:\n",
        "    if col not in test_cols:\n",
        "        processed_test_df[col] = 0\n",
        "for col in test_cols:\n",
        "    if col not in train_cols:\n",
        "        processed_train_df[col] = 0\n",
        "common_columns_ordered = processed_train_df.columns.tolist()\n",
        "processed_test_df = processed_test_df[common_columns_ordered]\n",
        "\n",
        "original_numerical_continuous_cols = [\n",
        "    'Facebook Popularity Quotient', 'Instagram Popularity Quotient',\n",
        "    'Restaurant Zomato Rating', 'Order Wait Time', 'Staff Responsivness',\n",
        "    'Value for Money', 'Hygiene Rating', 'Food Rating', 'Overall Restaurant Rating',\n",
        "    'Live Music Rating', 'Comedy Gigs Rating', 'Value Deals Rating', 'Live Sports Rating',\n",
        "    'Ambience', 'Lively', 'Service', 'Comfortablility', 'Privacy',\n",
        "    'Restaurant_Age_Days', 'Opening_Year', 'Opening_Month', 'Opening_DayOfWeek',\n",
        "    'Num_Cuisines']\n",
        "numerical_features_to_scale = [col for col in original_numerical_continuous_cols if col in processed_train_df.columns]\n",
        "\n",
        "if numerical_features_to_scale:\n",
        "    scaler = StandardScaler()\n",
        "    processed_train_df[numerical_features_to_scale] = scaler.fit_transform(processed_train_df[numerical_features_to_scale])\n",
        "    processed_test_df[numerical_features_to_scale] = scaler.transform(processed_test_df[numerical_features_to_scale])\n",
        "\n",
        "train_target_log = np.log1p(train_target)\n",
        "\n",
        "# --- Optuna Objective Function ---\n",
        "def objective(trial):\n",
        "    # Define hyperparameter search space\n",
        "    params = {\n",
        "        'objective': 'regression_l1', # Changed to L1 for potentially sparser solutions\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 1000, 5000), # Wider range for n_estimators\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05), # Slightly lower max LR\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100), # Wider range for num_leaves\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 15), # Explicit max_depth\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.9),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.9),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "        'min_split_gain': trial.suggest_float('min_split_gain', 1e-8, 1.0, log=True),\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': 42,\n",
        "        'boosting_type': 'gbdt',\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=123) # Using 5 splits for faster Optuna trials\n",
        "    oof_rmse_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(processed_train_df, train_target_log)):\n",
        "        X_train, y_train = processed_train_df.iloc[train_idx], train_target_log.iloc[train_idx]\n",
        "        X_val, y_val = processed_train_df.iloc[val_idx], train_target_log.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  eval_metric='rmse',\n",
        "                  callbacks=[lgb.early_stopping(100, verbose=False)]) # Reduced patience for early stopping\n",
        "\n",
        "        preds_val = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
        "        oof_rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(oof_rmse_scores)\n",
        "\n",
        "# --- Run Optuna Study ---\n",
        "study = optuna.create_study(direction='minimize', study_name='lgbm_turnover_regression')\n",
        "# Number of trials - can be increased for more thorough search\n",
        "study.optimize(objective, n_trials=60, timeout=1200) # 60 trials or 20 minutes timeout\n",
        "\n",
        "best_params = study.best_params\n",
        "print(\"Best hyperparameters found by Optuna:\", best_params)\n",
        "\n",
        "# --- Model Training with Best Parameters ---\n",
        "# Using K-Fold for final prediction averaging, as it often yields more robust results\n",
        "final_params_lgb = {\n",
        "    'objective': 'regression_l1',\n",
        "    'metric': 'rmse',\n",
        "    'verbose': -1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': 42,\n",
        "    'boosting_type': 'gbdt',\n",
        "}\n",
        "final_params_lgb.update(best_params) # Update with Optuna's best params\n",
        "\n",
        "kf_final = KFold(n_splits=10, shuffle=True, random_state=42) # Back to 10 splits for final model\n",
        "oof_predictions_final = np.zeros(len(processed_train_df))\n",
        "test_predictions_final = np.zeros(len(processed_test_df))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf_final.split(processed_train_df, train_target_log)):\n",
        "    X_train, y_train = processed_train_df.iloc[train_idx], train_target_log.iloc[train_idx]\n",
        "    X_val, y_val = processed_train_df.iloc[val_idx], train_target_log.iloc[val_idx]\n",
        "\n",
        "    model = lgb.LGBMRegressor(**final_params_lgb)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(150, verbose=False)]) # Longer patience for final model\n",
        "\n",
        "    oof_predictions_final[val_idx] = model.predict(X_val)\n",
        "    test_predictions_final += model.predict(processed_test_df) / kf_final.n_splits\n",
        "\n",
        "# --- Evaluation and Submission ---\n",
        "oof_rmse_log_final = np.sqrt(mean_squared_error(train_target_log, oof_predictions_final))\n",
        "print(f\"\\nFinal OOF RMSE (log-transformed target) with best params: {oof_rmse_log_final}\")\n",
        "\n",
        "oof_predictions_orig_scale_final = np.expm1(oof_predictions_final)\n",
        "oof_predictions_orig_scale_final[oof_predictions_orig_scale_final < 0] = 0\n",
        "oof_rmse_orig_final = np.sqrt(mean_squared_error(train_target, oof_predictions_orig_scale_final))\n",
        "print(f\"Final OOF RMSE (original scale) with best params: {oof_rmse_orig_final}\")\n",
        "\n",
        "final_predictions_log_tuned = test_predictions_final\n",
        "final_predictions_tuned = np.expm1(final_predictions_log_tuned)\n",
        "final_predictions_tuned[final_predictions_tuned < 0] = 0\n",
        "\n",
        "submission_df_tuned = pd.DataFrame({\n",
        "    'Registration Number': test_reg_nums,\n",
        "    'Annual Turnover': final_predictions_tuned\n",
        "})\n",
        "\n",
        "submission_df_tuned.to_csv(\"submission_tuned.csv\", index=False)\n",
        "print(\"Tuned submission file created: submission_tuned.csv\")\n",
        "\n",
        "if oof_rmse_orig_final < 12200000:\n",
        "    print(f\"\\nSuccess! The OOF RMSE ({oof_rmse_orig_final:.2f}) is below 12,200,000.\")\n",
        "else:\n",
        "    print(f\"\\nFurther tuning or feature engineering might be needed. Current OOF RMSE: {oof_rmse_orig_final:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvss4t8Orw_M",
        "outputId": "93641769-4178-4d77-ca8f-9793c1168874"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-98c1ecf2e960>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(median_val_train, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(median_val_train, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(-1, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(fill_value, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(fill_value, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(fill_value, inplace=True)\n",
            "<ipython-input-6-98c1ecf2e960>:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df[col].fillna(fill_value, inplace=True)\n",
            "[I 2025-06-08 22:35:34,137] A new study created in memory with name: lgbm_turnover_regression\n",
            "[I 2025-06-08 22:35:51,865] Trial 0 finished with value: 0.47416889342664126 and parameters: {'n_estimators': 2040, 'learning_rate': 0.01873155697116084, 'num_leaves': 29, 'max_depth': 8, 'min_child_samples': 55, 'feature_fraction': 0.537268916630564, 'bagging_fraction': 0.6394127360347078, 'bagging_freq': 7, 'lambda_l1': 6.67564464069512, 'lambda_l2': 0.3028985126235981, 'min_split_gain': 0.004175667710089106}. Best is trial 0 with value: 0.47416889342664126.\n",
            "[I 2025-06-08 22:35:57,858] Trial 1 finished with value: 0.4732439902708501 and parameters: {'n_estimators': 4920, 'learning_rate': 0.0494860125623701, 'num_leaves': 51, 'max_depth': 6, 'min_child_samples': 61, 'feature_fraction': 0.6475730157008315, 'bagging_fraction': 0.6756795753133297, 'bagging_freq': 7, 'lambda_l1': 1.47767277527792e-07, 'lambda_l2': 0.0003719506079492153, 'min_split_gain': 0.004081067990960856}. Best is trial 1 with value: 0.4732439902708501.\n",
            "[I 2025-06-08 22:36:00,482] Trial 2 finished with value: 0.4737812592629768 and parameters: {'n_estimators': 4099, 'learning_rate': 0.03156600702171994, 'num_leaves': 50, 'max_depth': 14, 'min_child_samples': 56, 'feature_fraction': 0.808205026039511, 'bagging_fraction': 0.5695715419937933, 'bagging_freq': 1, 'lambda_l1': 1.3025279535480673e-05, 'lambda_l2': 1.4303955036050858e-05, 'min_split_gain': 4.229175389431488e-08}. Best is trial 1 with value: 0.4732439902708501.\n",
            "[I 2025-06-08 22:36:09,188] Trial 3 finished with value: 0.47505649143839496 and parameters: {'n_estimators': 4216, 'learning_rate': 0.012846437700779183, 'num_leaves': 20, 'max_depth': 7, 'min_child_samples': 75, 'feature_fraction': 0.7074095191141616, 'bagging_fraction': 0.7219857317346076, 'bagging_freq': 2, 'lambda_l1': 4.3877559934782173e-07, 'lambda_l2': 6.34910434951281e-08, 'min_split_gain': 2.638240060822739e-06}. Best is trial 1 with value: 0.4732439902708501.\n",
            "[I 2025-06-08 22:36:28,957] Trial 4 finished with value: 0.47057917529050003 and parameters: {'n_estimators': 2559, 'learning_rate': 0.011902557003166234, 'num_leaves': 40, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.7168312341834083, 'bagging_fraction': 0.7873735622970313, 'bagging_freq': 4, 'lambda_l1': 2.0253418782312163, 'lambda_l2': 1.3704476941291898e-07, 'min_split_gain': 1.6026532547362681e-06}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:36:34,525] Trial 5 finished with value: 0.47602262638186144 and parameters: {'n_estimators': 1155, 'learning_rate': 0.01242663175663078, 'num_leaves': 27, 'max_depth': 11, 'min_child_samples': 82, 'feature_fraction': 0.741732547733192, 'bagging_fraction': 0.5730703860527114, 'bagging_freq': 4, 'lambda_l1': 2.392299684601697e-07, 'lambda_l2': 0.48685349936508676, 'min_split_gain': 0.004666774132032495}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:36:41,440] Trial 6 finished with value: 0.47560971763513643 and parameters: {'n_estimators': 3520, 'learning_rate': 0.04409528922695945, 'num_leaves': 75, 'max_depth': 14, 'min_child_samples': 81, 'feature_fraction': 0.8400631817254627, 'bagging_fraction': 0.7479251463043484, 'bagging_freq': 2, 'lambda_l1': 1.7537687907389704e-07, 'lambda_l2': 0.0265415568708713, 'min_split_gain': 0.25294587083678544}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:36:45,756] Trial 7 finished with value: 0.4753914392548918 and parameters: {'n_estimators': 1437, 'learning_rate': 0.030170264788602363, 'num_leaves': 66, 'max_depth': 7, 'min_child_samples': 86, 'feature_fraction': 0.6147184420672442, 'bagging_fraction': 0.8543931723590656, 'bagging_freq': 5, 'lambda_l1': 9.47750529983144e-08, 'lambda_l2': 0.28337956564535915, 'min_split_gain': 9.386742317646792e-07}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:36:50,036] Trial 8 finished with value: 0.4736356170069708 and parameters: {'n_estimators': 3118, 'learning_rate': 0.019921314952929862, 'num_leaves': 94, 'max_depth': 6, 'min_child_samples': 48, 'feature_fraction': 0.61353491671197, 'bagging_fraction': 0.5506190743431656, 'bagging_freq': 3, 'lambda_l1': 0.0001445549110863248, 'lambda_l2': 0.7196788864634684, 'min_split_gain': 7.868146623248455e-05}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:37:03,842] Trial 9 finished with value: 0.47127463553627846 and parameters: {'n_estimators': 4948, 'learning_rate': 0.010074681061682814, 'num_leaves': 69, 'max_depth': 6, 'min_child_samples': 43, 'feature_fraction': 0.5443461557183802, 'bagging_fraction': 0.6236046749322006, 'bagging_freq': 5, 'lambda_l1': 2.002326533388602e-08, 'lambda_l2': 0.061580669838316865, 'min_split_gain': 1.6896813747177377e-08}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:37:41,334] Trial 10 finished with value: 0.4721996736981259 and parameters: {'n_estimators': 2455, 'learning_rate': 0.005221514125684196, 'num_leaves': 41, 'max_depth': 11, 'min_child_samples': 5, 'feature_fraction': 0.8965910657892453, 'bagging_fraction': 0.8874893248210014, 'bagging_freq': 5, 'lambda_l1': 1.5269020819637384, 'lambda_l2': 2.2603306296953895e-08, 'min_split_gain': 1.2239353703159393e-05}. Best is trial 4 with value: 0.47057917529050003.\n",
            "[I 2025-06-08 22:38:23,572] Trial 11 finished with value: 0.46906559790036706 and parameters: {'n_estimators': 2634, 'learning_rate': 0.0052299681019404454, 'num_leaves': 76, 'max_depth': 9, 'min_child_samples': 8, 'feature_fraction': 0.508789138782842, 'bagging_fraction': 0.80309647141515, 'bagging_freq': 5, 'lambda_l1': 0.09486034447534025, 'lambda_l2': 0.0006924785916805033, 'min_split_gain': 1.4944296309426337e-08}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:39:10,424] Trial 12 finished with value: 0.4703671588120882 and parameters: {'n_estimators': 2427, 'learning_rate': 0.00547171338154381, 'num_leaves': 88, 'max_depth': 9, 'min_child_samples': 5, 'feature_fraction': 0.7589808470370067, 'bagging_fraction': 0.7991170660118846, 'bagging_freq': 4, 'lambda_l1': 0.10270049800470873, 'lambda_l2': 6.8147572616851745e-06, 'min_split_gain': 2.0761978170631547e-07}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:39:21,807] Trial 13 finished with value: 0.47309124918492057 and parameters: {'n_estimators': 1900, 'learning_rate': 0.021632193871668683, 'num_leaves': 87, 'max_depth': 9, 'min_child_samples': 23, 'feature_fraction': 0.5034466651345163, 'bagging_fraction': 0.8244382771615333, 'bagging_freq': 6, 'lambda_l1': 0.04574563656240798, 'lambda_l2': 5.5103824943793503e-05, 'min_split_gain': 1.2777873823407176e-07}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:39:55,086] Trial 14 finished with value: 0.4726160661313921 and parameters: {'n_estimators': 3039, 'learning_rate': 0.005524917740582832, 'num_leaves': 82, 'max_depth': 12, 'min_child_samples': 26, 'feature_fraction': 0.7744845307517344, 'bagging_fraction': 0.7912287709922499, 'bagging_freq': 3, 'lambda_l1': 0.03126879380113089, 'lambda_l2': 0.001967052958738156, 'min_split_gain': 2.0354453253734824e-07}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:40:04,442] Trial 15 finished with value: 0.4713991738227901 and parameters: {'n_estimators': 2541, 'learning_rate': 0.03429274626121055, 'num_leaves': 95, 'max_depth': 9, 'min_child_samples': 22, 'feature_fraction': 0.6585399892877484, 'bagging_fraction': 0.7743378157305761, 'bagging_freq': 6, 'lambda_l1': 0.01910228815555588, 'lambda_l2': 3.5105652274571836e-06, 'min_split_gain': 1.1252111019341778e-08}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:40:23,811] Trial 16 finished with value: 0.4718217624864033 and parameters: {'n_estimators': 1882, 'learning_rate': 0.02578585631882291, 'num_leaves': 80, 'max_depth': 12, 'min_child_samples': 15, 'feature_fraction': 0.7781097788076646, 'bagging_fraction': 0.8591385107066827, 'bagging_freq': 3, 'lambda_l1': 0.0010412222385132786, 'lambda_l2': 1.6344037968989584e-06, 'min_split_gain': 3.359315991186135e-05}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:40:27,391] Trial 17 finished with value: 0.4723728236280917 and parameters: {'n_estimators': 3630, 'learning_rate': 0.03674840177708848, 'num_leaves': 98, 'max_depth': 9, 'min_child_samples': 36, 'feature_fraction': 0.8641786072288296, 'bagging_fraction': 0.7091465890210678, 'bagging_freq': 6, 'lambda_l1': 0.1921350532212645, 'lambda_l2': 0.0026057685368041567, 'min_split_gain': 4.6842155367479793e-07}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:40:39,370] Trial 18 finished with value: 0.47054731512771425 and parameters: {'n_estimators': 2220, 'learning_rate': 0.016142197882724196, 'num_leaves': 57, 'max_depth': 5, 'min_child_samples': 36, 'feature_fraction': 0.6566715390912259, 'bagging_fraction': 0.8214830792719451, 'bagging_freq': 4, 'lambda_l1': 0.0004085690933485198, 'lambda_l2': 0.00013894510168032737, 'min_split_gain': 6.219902698757253e-06}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:41:03,154] Trial 19 finished with value: 0.4706265744906076 and parameters: {'n_estimators': 1496, 'learning_rate': 0.007801638364238702, 'num_leaves': 88, 'max_depth': 8, 'min_child_samples': 14, 'feature_fraction': 0.5792347940895992, 'bagging_fraction': 0.7597237033953899, 'bagging_freq': 5, 'lambda_l1': 0.003393174461844301, 'lambda_l2': 1.2123460065275159e-06, 'min_split_gain': 0.0002593922965163745}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:41:13,418] Trial 20 finished with value: 0.47231925650156387 and parameters: {'n_estimators': 2783, 'learning_rate': 0.0237361898811818, 'num_leaves': 72, 'max_depth': 10, 'min_child_samples': 14, 'feature_fraction': 0.7459740802068915, 'bagging_fraction': 0.6726213989054953, 'bagging_freq': 4, 'lambda_l1': 0.4031293759946337, 'lambda_l2': 4.3523825912692375, 'min_split_gain': 8.14680037348461e-08}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:41:22,615] Trial 21 finished with value: 0.4718086060972899 and parameters: {'n_estimators': 2194, 'learning_rate': 0.016411831230553113, 'num_leaves': 59, 'max_depth': 5, 'min_child_samples': 33, 'feature_fraction': 0.6650263649627921, 'bagging_fraction': 0.8285729461830998, 'bagging_freq': 4, 'lambda_l1': 7.11669893465973e-05, 'lambda_l2': 5.669653273995997e-05, 'min_split_gain': 3.868078961249017e-06}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:41:33,832] Trial 22 finished with value: 0.4753303822818745 and parameters: {'n_estimators': 3372, 'learning_rate': 0.015310799482813418, 'num_leaves': 57, 'max_depth': 5, 'min_child_samples': 97, 'feature_fraction': 0.6773424273398679, 'bagging_fraction': 0.8973872133281883, 'bagging_freq': 4, 'lambda_l1': 4.088856285843292e-06, 'lambda_l2': 0.000447424071004007, 'min_split_gain': 4.159967417422333e-07}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:41:52,906] Trial 23 finished with value: 0.47087735809122905 and parameters: {'n_estimators': 2255, 'learning_rate': 0.00852148509584539, 'num_leaves': 78, 'max_depth': 8, 'min_child_samples': 33, 'feature_fraction': 0.5755139290048359, 'bagging_fraction': 0.8138492902105228, 'bagging_freq': 3, 'lambda_l1': 0.003834831923607037, 'lambda_l2': 0.006892218506007579, 'min_split_gain': 0.00035565062870703657}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:42:10,802] Trial 24 finished with value: 0.46989291434078906 and parameters: {'n_estimators': 1655, 'learning_rate': 0.016178018432869718, 'num_leaves': 65, 'max_depth': 13, 'min_child_samples': 5, 'feature_fraction': 0.502889556458544, 'bagging_fraction': 0.5047472930714152, 'bagging_freq': 5, 'lambda_l1': 0.15948896852761776, 'lambda_l2': 5.392801842237871e-05, 'min_split_gain': 7.3156329887848714e-06}. Best is trial 11 with value: 0.46906559790036706.\n",
            "[I 2025-06-08 22:42:38,775] Trial 25 finished with value: 0.46896097731805286 and parameters: {'n_estimators': 1419, 'learning_rate': 0.008028559340273946, 'num_leaves': 86, 'max_depth': 15, 'min_child_samples': 5, 'feature_fraction': 0.5206781610791712, 'bagging_fraction': 0.5312144989276453, 'bagging_freq': 6, 'lambda_l1': 0.14290351011239869, 'lambda_l2': 7.5071048116587454e-06, 'min_split_gain': 4.416509775996483e-08}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:42:55,541] Trial 26 finished with value: 0.47216457708343196 and parameters: {'n_estimators': 1067, 'learning_rate': 0.009888218824464346, 'num_leaves': 64, 'max_depth': 15, 'min_child_samples': 14, 'feature_fraction': 0.5067930211819323, 'bagging_fraction': 0.5989152662727932, 'bagging_freq': 6, 'lambda_l1': 0.5465427003520473, 'lambda_l2': 3.451387447326562e-07, 'min_split_gain': 2.7008646463624103e-08}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:43:05,766] Trial 27 finished with value: 0.4726594444893818 and parameters: {'n_estimators': 1403, 'learning_rate': 0.01492073750672274, 'num_leaves': 84, 'max_depth': 15, 'min_child_samples': 21, 'feature_fraction': 0.5437037391692583, 'bagging_fraction': 0.5090763536043189, 'bagging_freq': 7, 'lambda_l1': 0.010915330883729894, 'lambda_l2': 2.3642880406229587e-05, 'min_split_gain': 0.3816010496290251}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:43:23,173] Trial 28 finished with value: 0.47328324012652895 and parameters: {'n_estimators': 1701, 'learning_rate': 0.008238221781298951, 'num_leaves': 74, 'max_depth': 13, 'min_child_samples': 11, 'feature_fraction': 0.5801065756593604, 'bagging_fraction': 0.5019471245042559, 'bagging_freq': 6, 'lambda_l1': 1.6142537367667227, 'lambda_l2': 0.0006319015624783324, 'min_split_gain': 5.847767882148974e-08}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:43:32,739] Trial 29 finished with value: 0.4731735046725964 and parameters: {'n_estimators': 1688, 'learning_rate': 0.019202744905589514, 'num_leaves': 64, 'max_depth': 14, 'min_child_samples': 28, 'feature_fraction': 0.5251492131868477, 'bagging_fraction': 0.5249174909173849, 'bagging_freq': 5, 'lambda_l1': 3.3758592184303273, 'lambda_l2': 4.6191240608687157e-07, 'min_split_gain': 0.0025617312175924543}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:43:38,103] Trial 30 finished with value: 0.4759040680870797 and parameters: {'n_estimators': 1242, 'learning_rate': 0.011819478134551671, 'num_leaves': 70, 'max_depth': 13, 'min_child_samples': 66, 'feature_fraction': 0.5578461589073406, 'bagging_fraction': 0.5357805537018874, 'bagging_freq': 7, 'lambda_l1': 6.701302142748673, 'lambda_l2': 6.32992577461118e-05, 'min_split_gain': 1.318947944953587e-06}. Best is trial 25 with value: 0.46896097731805286.\n",
            "[I 2025-06-08 22:44:18,103] Trial 31 finished with value: 0.46829759599797205 and parameters: {'n_estimators': 2781, 'learning_rate': 0.006035767136101405, 'num_leaves': 90, 'max_depth': 11, 'min_child_samples': 5, 'feature_fraction': 0.5258391983395648, 'bagging_fraction': 0.6176086714695753, 'bagging_freq': 5, 'lambda_l1': 0.13045330254074008, 'lambda_l2': 8.849103743391908e-06, 'min_split_gain': 1.871501611923595e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:44:47,054] Trial 32 finished with value: 0.47067305581613167 and parameters: {'n_estimators': 2859, 'learning_rate': 0.007363179209665699, 'num_leaves': 91, 'max_depth': 12, 'min_child_samples': 10, 'feature_fraction': 0.5004625621122779, 'bagging_fraction': 0.6336569381412739, 'bagging_freq': 5, 'lambda_l1': 0.12400066561297898, 'lambda_l2': 9.056106515456553e-06, 'min_split_gain': 1.0700983827248523e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:45:17,575] Trial 33 finished with value: 0.4722047630115286 and parameters: {'n_estimators': 1887, 'learning_rate': 0.005057500627131173, 'num_leaves': 99, 'max_depth': 13, 'min_child_samples': 20, 'feature_fraction': 0.6149611207584534, 'bagging_fraction': 0.6627581390531035, 'bagging_freq': 6, 'lambda_l1': 0.44109940484826593, 'lambda_l2': 0.00017011767830727386, 'min_split_gain': 4.6349163635614764e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:45:34,204] Trial 34 finished with value: 0.4691344656262196 and parameters: {'n_estimators': 3856, 'learning_rate': 0.01044932994781091, 'num_leaves': 52, 'max_depth': 11, 'min_child_samples': 9, 'feature_fraction': 0.5333621885375391, 'bagging_fraction': 0.5965435210573984, 'bagging_freq': 5, 'lambda_l1': 0.00783952528792618, 'lambda_l2': 1.7377433551177438e-05, 'min_split_gain': 3.749725801001139e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:45:55,212] Trial 35 finished with value: 0.47073639550760127 and parameters: {'n_estimators': 4393, 'learning_rate': 0.010004237574406489, 'num_leaves': 49, 'max_depth': 11, 'min_child_samples': 18, 'feature_fraction': 0.5312667148138692, 'bagging_fraction': 0.5963155571801899, 'bagging_freq': 6, 'lambda_l1': 0.005355978114316255, 'lambda_l2': 2.541427891561403e-06, 'min_split_gain': 2.1581474638051146e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:10,922] Trial 36 finished with value: 0.46904779913631217 and parameters: {'n_estimators': 4042, 'learning_rate': 0.011503910583207318, 'num_leaves': 42, 'max_depth': 10, 'min_child_samples': 10, 'feature_fraction': 0.5650834322553159, 'bagging_fraction': 0.5767812481239186, 'bagging_freq': 7, 'lambda_l1': 0.0006726749510304099, 'lambda_l2': 1.2704533475195289e-05, 'min_split_gain': 3.818425294444184e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:17,685] Trial 37 finished with value: 0.47083103713186675 and parameters: {'n_estimators': 4526, 'learning_rate': 0.013744975499908559, 'num_leaves': 29, 'max_depth': 8, 'min_child_samples': 28, 'feature_fraction': 0.5611803672059181, 'bagging_fraction': 0.5502113600435053, 'bagging_freq': 7, 'lambda_l1': 8.769891099297478e-06, 'lambda_l2': 5.510657533217129e-07, 'min_split_gain': 3.406866266082256e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:23,523] Trial 38 finished with value: 0.47538232744115394 and parameters: {'n_estimators': 3318, 'learning_rate': 0.0469150981669232, 'num_leaves': 35, 'max_depth': 10, 'min_child_samples': 61, 'feature_fraction': 0.6043259141612516, 'bagging_fraction': 0.5712482931081297, 'bagging_freq': 7, 'lambda_l1': 0.0008614644732992601, 'lambda_l2': 0.0008015573851985989, 'min_split_gain': 7.307670737318466e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:30,602] Trial 39 finished with value: 0.4732551138688644 and parameters: {'n_estimators': 3936, 'learning_rate': 0.03962508957323589, 'num_leaves': 77, 'max_depth': 10, 'min_child_samples': 9, 'feature_fraction': 0.5924031837954845, 'bagging_fraction': 0.7310196429613113, 'bagging_freq': 7, 'lambda_l1': 7.382168815857447e-05, 'lambda_l2': 1.2519652113549096e-07, 'min_split_gain': 8.244688444728871e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:41,580] Trial 40 finished with value: 0.47122673050508296 and parameters: {'n_estimators': 4676, 'learning_rate': 0.01297781564534516, 'num_leaves': 46, 'max_depth': 7, 'min_child_samples': 44, 'feature_fraction': 0.6378586823356501, 'bagging_fraction': 0.6527077529381977, 'bagging_freq': 1, 'lambda_l1': 2.0974431822185245e-06, 'lambda_l2': 5.877265436597309e-06, 'min_split_gain': 2.114182359696085e-08}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:46:58,898] Trial 41 finished with value: 0.46972368663108144 and parameters: {'n_estimators': 3771, 'learning_rate': 0.0111091284909202, 'num_leaves': 44, 'max_depth': 11, 'min_child_samples': 10, 'feature_fraction': 0.5226901821608537, 'bagging_fraction': 0.6014904936134134, 'bagging_freq': 6, 'lambda_l1': 0.05406382508918939, 'lambda_l2': 1.2877947770772937e-05, 'min_split_gain': 5.080399546181241e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:47:22,571] Trial 42 finished with value: 0.47046578693085167 and parameters: {'n_estimators': 3896, 'learning_rate': 0.007350707266534182, 'num_leaves': 53, 'max_depth': 11, 'min_child_samples': 17, 'feature_fraction': 0.5614695928642585, 'bagging_fraction': 0.690605213150209, 'bagging_freq': 5, 'lambda_l1': 0.001324174309168043, 'lambda_l2': 2.0416532518140873e-05, 'min_split_gain': 1.3606046273804416e-07}. Best is trial 31 with value: 0.46829759599797205.\n",
            "[I 2025-06-08 22:47:39,470] Trial 43 finished with value: 0.46758814089585937 and parameters: {'n_estimators': 4133, 'learning_rate': 0.010327178165746173, 'num_leaves': 33, 'max_depth': 10, 'min_child_samples': 7, 'feature_fraction': 0.5408741214324725, 'bagging_fraction': 0.6115152005115341, 'bagging_freq': 5, 'lambda_l1': 0.009123122409209202, 'lambda_l2': 0.0002539884797728875, 'min_split_gain': 0.055213605627062755}. Best is trial 43 with value: 0.46758814089585937.\n",
            "[I 2025-06-08 22:47:57,663] Trial 44 finished with value: 0.4672836819605001 and parameters: {'n_estimators': 4175, 'learning_rate': 0.0071555998771090455, 'num_leaves': 22, 'max_depth': 10, 'min_child_samples': 8, 'feature_fraction': 0.5471487479142861, 'bagging_fraction': 0.5614118994206465, 'bagging_freq': 6, 'lambda_l1': 0.01667949466079257, 'lambda_l2': 0.012432291348663504, 'min_split_gain': 0.013168564451584682}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:03,012] Trial 45 finished with value: 0.4687489316920607 and parameters: {'n_estimators': 4253, 'learning_rate': 0.029608749783665397, 'num_leaves': 21, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.5522962036680105, 'bagging_fraction': 0.5547703722545836, 'bagging_freq': 6, 'lambda_l1': 0.018858757497932723, 'lambda_l2': 0.02135706974847719, 'min_split_gain': 0.06913962759008485}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:09,168] Trial 46 finished with value: 0.47127770272394826 and parameters: {'n_estimators': 4237, 'learning_rate': 0.028986999280719385, 'num_leaves': 26, 'max_depth': 10, 'min_child_samples': 18, 'feature_fraction': 0.634293516028291, 'bagging_fraction': 0.6201321365428413, 'bagging_freq': 6, 'lambda_l1': 0.020265964384297567, 'lambda_l2': 0.04253144405058305, 'min_split_gain': 0.08410148989562029}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:13,556] Trial 47 finished with value: 0.4677187961540256 and parameters: {'n_estimators': 4163, 'learning_rate': 0.03138580650593934, 'num_leaves': 20, 'max_depth': 12, 'min_child_samples': 5, 'feature_fraction': 0.5461514995401997, 'bagging_fraction': 0.5584941939652921, 'bagging_freq': 6, 'lambda_l1': 0.001991033449289511, 'lambda_l2': 0.013319999381937803, 'min_split_gain': 0.047110959545225886}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:20,740] Trial 48 finished with value: 0.4707635438861114 and parameters: {'n_estimators': 4704, 'learning_rate': 0.03002140518870023, 'num_leaves': 21, 'max_depth': 12, 'min_child_samples': 24, 'feature_fraction': 0.5438629884215007, 'bagging_fraction': 0.5607393389693905, 'bagging_freq': 5, 'lambda_l1': 0.01679078565895303, 'lambda_l2': 0.01285987908302488, 'min_split_gain': 0.04928709362566617}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:24,830] Trial 49 finished with value: 0.47042378874623614 and parameters: {'n_estimators': 4099, 'learning_rate': 0.03282966921025447, 'num_leaves': 35, 'max_depth': 11, 'min_child_samples': 14, 'feature_fraction': 0.597736988510764, 'bagging_fraction': 0.5810514254177592, 'bagging_freq': 6, 'lambda_l1': 0.0020936117923240367, 'lambda_l2': 0.11123274994003357, 'min_split_gain': 0.008031609937064673}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:34,436] Trial 50 finished with value: 0.4688137871791584 and parameters: {'n_estimators': 4299, 'learning_rate': 0.02588727932423797, 'num_leaves': 20, 'max_depth': 12, 'min_child_samples': 5, 'feature_fraction': 0.7108582642928651, 'bagging_fraction': 0.6129536808023136, 'bagging_freq': 5, 'lambda_l1': 0.039965121695434945, 'lambda_l2': 0.2063782966772467, 'min_split_gain': 0.016357895258249468}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:39,550] Trial 51 finished with value: 0.468842147982377 and parameters: {'n_estimators': 4431, 'learning_rate': 0.0256232448819493, 'num_leaves': 22, 'max_depth': 12, 'min_child_samples': 5, 'feature_fraction': 0.5431223438380982, 'bagging_fraction': 0.6415972005723035, 'bagging_freq': 5, 'lambda_l1': 0.047301257069343856, 'lambda_l2': 0.20558428229135414, 'min_split_gain': 0.01318538244096643}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:42,878] Trial 52 finished with value: 0.4714270417989656 and parameters: {'n_estimators': 4207, 'learning_rate': 0.040028643805813835, 'num_leaves': 24, 'max_depth': 11, 'min_child_samples': 12, 'feature_fraction': 0.7167261553235159, 'bagging_fraction': 0.6209490047874429, 'bagging_freq': 6, 'lambda_l1': 0.00021390708930376205, 'lambda_l2': 0.00729234212326242, 'min_split_gain': 0.07857481025195351}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:50,489] Trial 53 finished with value: 0.4691394433420153 and parameters: {'n_estimators': 4721, 'learning_rate': 0.035167592736172565, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 7, 'feature_fraction': 0.794013982030143, 'bagging_fraction': 0.546995825608796, 'bagging_freq': 5, 'lambda_l1': 0.009535246051787903, 'lambda_l2': 0.002306737067761624, 'min_split_gain': 0.02481028959791212}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:48:55,483] Trial 54 finished with value: 0.4714828543713918 and parameters: {'n_estimators': 3579, 'learning_rate': 0.026662304727389993, 'num_leaves': 35, 'max_depth': 9, 'min_child_samples': 16, 'feature_fraction': 0.6836119818415372, 'bagging_fraction': 0.6128134203366469, 'bagging_freq': 4, 'lambda_l1': 0.002065545031831112, 'lambda_l2': 0.7931049539263856, 'min_split_gain': 0.0014820184240360687}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:49:03,021] Trial 55 finished with value: 0.4754954544198605 and parameters: {'n_estimators': 4284, 'learning_rate': 0.021896074836560683, 'num_leaves': 20, 'max_depth': 10, 'min_child_samples': 71, 'feature_fraction': 0.7219402816697702, 'bagging_fraction': 0.5648030311496068, 'bagging_freq': 6, 'lambda_l1': 0.7941011626910471, 'lambda_l2': 0.01421015156250141, 'min_split_gain': 0.15027443306726065}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:49:06,969] Trial 56 finished with value: 0.4701128721679548 and parameters: {'n_estimators': 4572, 'learning_rate': 0.03204018804322657, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 7, 'feature_fraction': 0.6246074997788834, 'bagging_fraction': 0.5231788887732862, 'bagging_freq': 5, 'lambda_l1': 0.03419195483997848, 'lambda_l2': 1.8267108115425355, 'min_split_gain': 0.5077869768630513}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:49:12,227] Trial 57 finished with value: 0.46927350360347264 and parameters: {'n_estimators': 3194, 'learning_rate': 0.02817568098179476, 'num_leaves': 24, 'max_depth': 9, 'min_child_samples': 12, 'feature_fraction': 0.6884990074106749, 'bagging_fraction': 0.5841250987360588, 'bagging_freq': 5, 'lambda_l1': 0.06787547631890863, 'lambda_l2': 0.09323844830436603, 'min_split_gain': 0.031634808272982325}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:49:20,009] Trial 58 finished with value: 0.4745120263811744 and parameters: {'n_estimators': 3635, 'learning_rate': 0.02406292702015152, 'num_leaves': 27, 'max_depth': 11, 'min_child_samples': 51, 'feature_fraction': 0.5815968410925378, 'bagging_fraction': 0.5558679496554717, 'bagging_freq': 6, 'lambda_l1': 0.004194651150870813, 'lambda_l2': 0.025915061179705975, 'min_split_gain': 0.0008532260562270569}. Best is trial 44 with value: 0.4672836819605001.\n",
            "[I 2025-06-08 22:49:24,793] Trial 59 finished with value: 0.4720391815291463 and parameters: {'n_estimators': 4094, 'learning_rate': 0.037276357006012145, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 19, 'feature_fraction': 0.5523524474472652, 'bagging_fraction': 0.6489328587380813, 'bagging_freq': 4, 'lambda_l1': 0.017617519092289435, 'lambda_l2': 0.34789081823989004, 'min_split_gain': 0.1678464856458834}. Best is trial 44 with value: 0.4672836819605001.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by Optuna: {'n_estimators': 4175, 'learning_rate': 0.0071555998771090455, 'num_leaves': 22, 'max_depth': 10, 'min_child_samples': 8, 'feature_fraction': 0.5471487479142861, 'bagging_fraction': 0.5614118994206465, 'bagging_freq': 6, 'lambda_l1': 0.01667949466079257, 'lambda_l2': 0.012432291348663504, 'min_split_gain': 0.013168564451584682}\n",
            "\n",
            "Final OOF RMSE (log-transformed target) with best params: 0.46693251755879744\n",
            "Final OOF RMSE (original scale) with best params: 20137812.374023587\n",
            "Tuned submission file created: submission_tuned.csv\n",
            "\n",
            "Further tuning or feature engineering might be needed. Current OOF RMSE: 20137812.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- XGBoost Model (Added) ---\n",
        "# Optuna Objective for XGBoost\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror', # Use RMSE objective\n",
        "        'eval_metric': 'rmse',\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 1000, 5000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True), # L2 regularization\n",
        "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),  # L1 regularization\n",
        "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),     # Minimum loss reduction required to make a split\n",
        "        'seed': 42,\n",
        "        'n_jobs': -1,\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    oof_rmse_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(processed_train_df, train_target_log)):\n",
        "        X_train, y_train = processed_train_df.iloc[train_idx], train_target_log.iloc[train_idx]\n",
        "        X_val, y_val = processed_train_df.iloc[val_idx], train_target_log.iloc[val_idx]\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  verbose=False\n",
        "                 )\n",
        "\n",
        "        preds_val = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
        "        oof_rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(oof_rmse_scores)\n",
        "\n",
        "print(\"\\nStarting Optuna study for XGBoost...\")\n",
        "study_xgb = optuna.create_study(direction='minimize', study_name='xgb_turnover_regression')\n",
        "study_xgb.optimize(objective_xgb, n_trials=60, timeout=1200) # Run 60 trials or for 20 minutes\n",
        "\n",
        "best_params_xgb = study_xgb.best_params\n",
        "print(\"Best hyperparameters found by Optuna for XGBoost:\", best_params_xgb)\n",
        "\n",
        "# --- CatBoost Model (Added) ---\n",
        "# Optuna Objective for CatBoost\n",
        "def objective_cat(trial):\n",
        "    params = {\n",
        "        'objective': 'RMSE',\n",
        "        'iterations': trial.suggest_int('iterations', 1000, 5000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),\n",
        "        'depth': trial.suggest_int('depth', 5, 10), # CatBoost depth typically lower\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
        "        'random_seed': 42,\n",
        "        'verbose': 0, # Suppress verbose output during trials\n",
        "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 200),\n",
        "        # Removed 'Poisson' as it's not supported on CPU\n",
        "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS', 'No']),\n",
        "    }\n",
        "\n",
        "    # Conditionally suggest bagging_temperature and subsample based on bootstrap_type\n",
        "    bootstrap_type = trial.params['bootstrap_type']\n",
        "    if bootstrap_type == 'Bayesian':\n",
        "         params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 1)\n",
        "    elif bootstrap_type in ['Bernoulli', 'MVS']:\n",
        "         params['subsample'] = trial.suggest_float('subsample', 0.5, 0.9)\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    oof_rmse_scores = []\n",
        "\n",
        "    # CatBoost requires specifying categorical features explicitly\n",
        "    # Need to reverse one-hot encoding to find original categorical columns for CatBoost\n",
        "    # Note: This requires knowing the original column names before OHE\n",
        "    # For simplicity here, let's assume no explicit categorical feature handling in CatBoost for now,\n",
        "    # as it works well with numeric features. If performance is poor, this is a place to optimize.\n",
        "    # CatBoost can handle integer-encoded categorical features directly. Our OHE prevents this.\n",
        "    # If we wanted to use CatBoost's categorical feature handling, we'd need to use a different preprocessing path for it.\n",
        "    # For now, we'll treat all features the same as for LGBM and XGBoost.\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(processed_train_df, train_target_log)):\n",
        "        X_train, y_train = processed_train_df.iloc[train_idx], train_target_log.iloc[train_idx]\n",
        "        X_val, y_val = processed_train_df.iloc[val_idx], train_target_log.iloc[val_idx]\n",
        "\n",
        "        model = CatBoostRegressor(**params)\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  early_stopping_rounds=params['early_stopping_rounds'],\n",
        "                  verbose=0)\n",
        "\n",
        "        preds_val = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
        "        oof_rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(oof_rmse_scores)\n",
        "\n",
        "print(\"\\nStarting Optuna study for CatBoost...\")\n",
        "study_cat = optuna.create_study(direction='minimize', study_name='cat_turnover_regression')\n",
        "study_cat.optimize(objective_cat, n_trials=60, timeout=1200) # Run 60 trials or for 20 minutes\n",
        "\n",
        "best_params_cat = study_cat.best_params\n",
        "print(\"Best hyperparameters found by Optuna for CatBoost:\", best_params_cat)\n",
        "\n",
        "\n",
        "# --- Final Model Training and Averaging ---\n",
        "kf_ensemble = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds_lgbm = np.zeros(len(processed_train_df))\n",
        "test_preds_lgbm = np.zeros(len(processed_test_df))\n",
        "oof_preds_xgb = np.zeros(len(processed_train_df))\n",
        "test_preds_xgb = np.zeros(len(processed_test_df))\n",
        "oof_preds_cat = np.zeros(len(processed_train_df))\n",
        "test_preds_cat = np.zeros(len(processed_test_df))\n",
        "\n",
        "lgbm_oof_rmses = []\n",
        "xgb_oof_rmses = []\n",
        "cat_oof_rmses = []\n",
        "\n",
        "print(\"\\nTraining base models and collecting OOF and Test predictions...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(kf_ensemble.split(processed_train_df, train_target_log)):\n",
        "    print(f\"--- Fold {fold+1} ---\")\n",
        "    X_train, y_train = processed_train_df.iloc[train_idx], train_target_log.iloc[train_idx]\n",
        "    X_val, y_val = processed_train_df.iloc[val_idx], train_target_log.iloc[val_idx]\n",
        "    X_test = processed_test_df.copy() # Use the entire test set for each fold's test prediction\n",
        "\n",
        "    # LightGBM\n",
        "    print(\"  Training LightGBM...\")\n",
        "    lgbm_model = lgb.LGBMRegressor(**final_params_lgb) # Use best params from initial Optuna\n",
        "    lgbm_model.fit(X_train, y_train,\n",
        "                   eval_set=[(X_val, y_val)],\n",
        "                   eval_metric='rmse',\n",
        "                   callbacks=[lgb.early_stopping(150, verbose=False)])\n",
        "    oof_preds_lgbm[val_idx] = lgbm_model.predict(X_val)\n",
        "    test_preds_lgbm += lgbm_model.predict(X_test) / kf_ensemble.n_splits\n",
        "\n",
        "    # XGBoost\n",
        "    print(\"  Training XGBoost...\")\n",
        "    xgb_model = xgb.XGBRegressor(**best_params_xgb) # Use best params from XGBoost Optuna\n",
        "    xgb_model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  verbose=False,\n",
        "                 )\n",
        "    oof_preds_xgb[val_idx] = xgb_model.predict(X_val)\n",
        "    test_preds_xgb += xgb_model.predict(X_test) / kf_ensemble.n_splits\n",
        "\n",
        "    # CatBoost\n",
        "    print(\"  Training CatBoost...\")\n",
        "    cat_model = CatBoostRegressor(**best_params_cat) # Use best params from CatBoost Optuna\n",
        "    # Need to handle categorical features for CatBoost if not using OHE, but we used OHE.\n",
        "    # So, fit on processed_train_df which has OHE features.\n",
        "    cat_model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  early_stopping_rounds=best_params_cat['early_stopping_rounds'],\n",
        "                  verbose=0)\n",
        "    oof_preds_cat[val_idx] = cat_model.predict(X_val)\n",
        "    test_preds_cat += cat_model.predict(X_test) / kf_ensemble.n_splits\n",
        "\n",
        "\n",
        "# Calculate OOF RMSE for each model (on original scale)\n",
        "oof_preds_lgbm_orig = np.expm1(oof_preds_lgbm)\n",
        "oof_preds_lgbm_orig[oof_preds_lgbm_orig < 0] = 0\n",
        "rmse_lgbm_oof = np.sqrt(mean_squared_error(train_target, oof_preds_lgbm_orig))\n",
        "print(f\"\\nLightGBM OOF RMSE (original scale): {rmse_lgbm_oof:.2f}\")\n",
        "\n",
        "oof_preds_xgb_orig = np.expm1(oof_preds_xgb)\n",
        "oof_preds_xgb_orig[oof_preds_xgb_orig < 0] = 0\n",
        "# Corrected variable name from 'of_preds_xgb_orig' to 'oof_preds_xgb_orig'\n",
        "rmse_xgb_oof = np.sqrt(mean_squared_error(train_target, oof_preds_xgb_orig))\n",
        "print(f\"XGBoost OOF RMSE (original scale): {rmse_xgb_oof:.2f}\")\n",
        "\n",
        "oof_preds_cat_orig = np.expm1(oof_preds_cat)\n",
        "oof_preds_cat_orig[oof_preds_cat_orig < 0] = 0\n",
        "rmse_cat_oof = np.sqrt(mean_squared_error(train_target, oof_preds_cat_orig))\n",
        "print(f\"CatBoost OOF RMSE (original scale): {rmse_cat_oof:.2f}\")\n",
        "\n",
        "# Calculate weights based on inverse of OOF RMSE\n",
        "# Add a small epsilon to avoid division by zero if RMSE is extremely close to zero\n",
        "epsilon = 1e-6\n",
        "weight_lgbm = 1 / (rmse_lgbm_oof + epsilon)\n",
        "weight_xgb = 1 / (rmse_xgb_oof + epsilon)\n",
        "weight_cat = 1 / (rmse_cat_oof + epsilon)\n",
        "\n",
        "total_weight = weight_lgbm + weight_xgb + weight_cat\n",
        "\n",
        "normalized_weight_lgbm = weight_lgbm / total_weight\n",
        "normalized_weight_xgb = weight_xgb / total_weight\n",
        "normalized_weight_cat = weight_cat / total_weight\n",
        "\n",
        "print(f\"\\nWeights for ensemble:\")\n",
        "print(f\"  LightGBM: {normalized_weight_lgbm:.4f}\")\n",
        "print(f\"  XGBoost:  {normalized_weight_xgb:.4f}\")\n",
        "print(f\"  CatBoost: {normalized_weight_cat:.4f}\")\n",
        "\n",
        "# Combine test predictions using weighted average (on log scale, then expm1)\n",
        "ensemble_test_predictions_log = (normalized_weight_lgbm * test_preds_lgbm +\n",
        "                                normalized_weight_xgb * test_preds_xgb +\n",
        "                                normalized_weight_cat * test_preds_cat)\n",
        "\n",
        "# Convert back to original scale\n",
        "ensemble_test_predictions_orig = np.expm1(ensemble_test_predictions_log)\n",
        "ensemble_test_predictions_orig[ensemble_test_predictions_orig < 0] = 0 # Ensure no negative predictions\n",
        "\n",
        "# --- Create Submission File ---\n",
        "submission_df_ensemble = pd.DataFrame({\n",
        "    'Registration Number': test_reg_nums,\n",
        "    'Annual Turnover': ensemble_test_predictions_orig\n",
        "})\n",
        "\n",
        "submission_df_ensemble.to_csv(\"submission_ensemble.csv\", index=False)\n",
        "print(\"\\nEnsemble submission file created: submission_ensemble.csv\")\n",
        "\n",
        "# Evaluate ensemble OOF RMSE (optional, but good for confidence)\n",
        "ensemble_oof_predictions_log = (normalized_weight_lgbm * oof_preds_lgbm +\n",
        "                               normalized_weight_xgb * oof_preds_xgb +\n",
        "                               normalized_weight_cat * oof_preds_cat)\n",
        "ensemble_oof_predictions_orig = np.expm1(ensemble_oof_predictions_log)\n",
        "ensemble_oof_predictions_orig[ensemble_oof_predictions_orig < 0] = 0\n",
        "ensemble_oof_rmse_orig = np.sqrt(mean_squared_error(train_target, ensemble_oof_predictions_orig))\n",
        "\n",
        "print(f\"\\nEnsemble OOF RMSE (original scale): {ensemble_oof_rmse_orig:.2f}\")\n",
        "\n",
        "if ensemble_oof_rmse_orig < 12200000:\n",
        "    print(f\"\\nSuccess! The Ensemble OOF RMSE ({ensemble_oof_rmse_orig:.2f}) is below 12,200,000.\")\n",
        "else:\n",
        "    print(f\"\\nFurther tuning, feature engineering, or different models might be needed. Current Ensemble OOF RMSE: {ensemble_oof_rmse_orig:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLQSjzCZr1Gc",
        "outputId": "bcd81603-d6c4-48b9-cca7-67aa43dc29fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-08 23:08:15,129] A new study created in memory with name: xgb_turnover_regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Optuna study for XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-08 23:09:39,287] Trial 0 finished with value: 0.4773475260318091 and parameters: {'n_estimators': 4252, 'learning_rate': 0.030806574522629707, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.8804380332862003, 'colsample_bytree': 0.7731078308007346, 'lambda': 0.003830588863888105, 'alpha': 4.477770836096484e-07, 'gamma': 0.03550707990395842}. Best is trial 0 with value: 0.4773475260318091.\n",
            "[I 2025-06-08 23:10:20,755] Trial 1 finished with value: 0.48960276702979366 and parameters: {'n_estimators': 1310, 'learning_rate': 0.049123199998764, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.5158745036483404, 'colsample_bytree': 0.7145644806987477, 'lambda': 0.38836990274368566, 'alpha': 0.0009572261514445418, 'gamma': 2.212305670349842e-07}. Best is trial 0 with value: 0.4773475260318091.\n",
            "[I 2025-06-08 23:12:06,321] Trial 2 finished with value: 0.4911988037571239 and parameters: {'n_estimators': 4159, 'learning_rate': 0.03382375119073215, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.502829694668192, 'colsample_bytree': 0.8832420555011121, 'lambda': 3.0727164853381517, 'alpha': 0.9240312456314084, 'gamma': 6.623668675257178e-08}. Best is trial 0 with value: 0.4773475260318091.\n",
            "[I 2025-06-08 23:12:51,684] Trial 3 finished with value: 0.4695275925348529 and parameters: {'n_estimators': 3540, 'learning_rate': 0.022941575291523242, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.7409412152210579, 'colsample_bytree': 0.662437726036701, 'lambda': 4.413666153700936e-05, 'alpha': 0.0020838006725489955, 'gamma': 0.5166150005476539}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:13:20,401] Trial 4 finished with value: 0.4773617187055981 and parameters: {'n_estimators': 1523, 'learning_rate': 0.02019074047291949, 'max_depth': 12, 'min_child_weight': 2, 'subsample': 0.8868433328690928, 'colsample_bytree': 0.8133850802692337, 'lambda': 0.00024431287948409525, 'alpha': 1.139770865488506e-05, 'gamma': 0.10187683665071927}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:15:32,502] Trial 5 finished with value: 0.4882771883025345 and parameters: {'n_estimators': 4866, 'learning_rate': 0.020216054362287492, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.6460316204903762, 'colsample_bytree': 0.6577815238264797, 'lambda': 3.8232852363990113, 'alpha': 0.9823719649825223, 'gamma': 0.0011925503346825299}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:17:35,196] Trial 6 finished with value: 0.47746731132308345 and parameters: {'n_estimators': 1995, 'learning_rate': 0.014894811548617357, 'max_depth': 13, 'min_child_weight': 2, 'subsample': 0.8439818708641401, 'colsample_bytree': 0.7347127436815329, 'lambda': 1.1125640091301399e-05, 'alpha': 0.01116850608741646, 'gamma': 2.4403197520822977e-08}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:19:10,541] Trial 7 finished with value: 0.4858150548751823 and parameters: {'n_estimators': 4310, 'learning_rate': 0.024566223914174228, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.6363432509016871, 'colsample_bytree': 0.5247885037382234, 'lambda': 0.0012488382986436729, 'alpha': 7.392078380029138e-07, 'gamma': 2.1262195357587218e-05}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:20:04,959] Trial 8 finished with value: 0.48084732300335276 and parameters: {'n_estimators': 3901, 'learning_rate': 0.030325275107906318, 'max_depth': 15, 'min_child_weight': 7, 'subsample': 0.8051310735711528, 'colsample_bytree': 0.8948641403057042, 'lambda': 4.244708223189976e-07, 'alpha': 2.9355966798866755e-07, 'gamma': 0.043174078609035105}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:21:43,476] Trial 9 finished with value: 0.48207926355978153 and parameters: {'n_estimators': 4437, 'learning_rate': 0.02754852445926529, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.8280810576661268, 'colsample_bytree': 0.8559601118203712, 'lambda': 3.736301874525289e-07, 'alpha': 0.0016487165934463497, 'gamma': 5.872177136703582e-06}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:22:57,836] Trial 10 finished with value: 0.4757687877295583 and parameters: {'n_estimators': 3016, 'learning_rate': 0.006393695461597324, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.7471741672679064, 'colsample_bytree': 0.620791008039319, 'lambda': 1.6150213776779762e-05, 'alpha': 4.747396375941625e-05, 'gamma': 0.0006321469219856617}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:24:08,975] Trial 11 finished with value: 0.4740665284651883 and parameters: {'n_estimators': 2857, 'learning_rate': 0.005484933986606785, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.731193595172519, 'colsample_bytree': 0.6158274745557429, 'lambda': 2.6961266303016942e-05, 'alpha': 5.501809624128334e-05, 'gamma': 0.0005642762122624267}. Best is trial 3 with value: 0.4695275925348529.\n",
            "[I 2025-06-08 23:24:57,832] Trial 12 finished with value: 0.46873537435166357 and parameters: {'n_estimators': 3099, 'learning_rate': 0.007401776756606139, 'max_depth': 8, 'min_child_weight': 9, 'subsample': 0.7159625137855314, 'colsample_bytree': 0.5763287393122055, 'lambda': 1.1537758843945419e-08, 'alpha': 0.02984128479135805, 'gamma': 0.37678777773522404}. Best is trial 12 with value: 0.46873537435166357.\n",
            "[I 2025-06-08 23:25:37,703] Trial 13 finished with value: 0.4707101782403976 and parameters: {'n_estimators': 3249, 'learning_rate': 0.042276320690675426, 'max_depth': 5, 'min_child_weight': 8, 'subsample': 0.6652709986215979, 'colsample_bytree': 0.5286262702938357, 'lambda': 3.157752087050276e-08, 'alpha': 0.10299494032746706, 'gamma': 0.6954397120503929}. Best is trial 12 with value: 0.46873537435166357.\n",
            "[I 2025-06-08 23:26:14,512] Trial 14 finished with value: 0.4680001139668571 and parameters: {'n_estimators': 2407, 'learning_rate': 0.012757899590413468, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.5879202675558062, 'colsample_bytree': 0.5758936540041081, 'lambda': 1.4174812478215432e-08, 'alpha': 0.01736218570178793, 'gamma': 0.9136973898476178}. Best is trial 14 with value: 0.4680001139668571.\n",
            "[I 2025-06-08 23:27:14,781] Trial 15 finished with value: 0.4819676925375848 and parameters: {'n_estimators': 2323, 'learning_rate': 0.014116374839025595, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.5745207091945006, 'colsample_bytree': 0.5745480979192596, 'lambda': 1.4572461879753105e-08, 'alpha': 0.03537254779031754, 'gamma': 0.008486214576514632}. Best is trial 14 with value: 0.4680001139668571.\n",
            "[I 2025-06-08 23:27:46,019] Trial 16 finished with value: 0.4749974120149602 and parameters: {'n_estimators': 2399, 'learning_rate': 0.012194671126257738, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.587740038282287, 'colsample_bytree': 0.5918946154939257, 'lambda': 3.028119133637111e-07, 'alpha': 7.357360145239964, 'gamma': 0.9126340111646004}. Best is trial 14 with value: 0.4680001139668571.\n",
            "[I 2025-06-08 23:28:55,996] Trial 17 finished with value: 0.4781876036846219 and parameters: {'n_estimators': 2582, 'learning_rate': 0.01082698219061993, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.5944354883254614, 'colsample_bytree': 0.5005811563830697, 'lambda': 1.0111030024271635e-08, 'alpha': 1.7383764104079603e-08, 'gamma': 0.005498654181064584}. Best is trial 14 with value: 0.4680001139668571.\n",
            "[I 2025-06-08 23:28:56,001] A new study created in memory with name: cat_turnover_regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by Optuna for XGBoost: {'n_estimators': 2407, 'learning_rate': 0.012757899590413468, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.5879202675558062, 'colsample_bytree': 0.5758936540041081, 'lambda': 1.4174812478215432e-08, 'alpha': 0.01736218570178793, 'gamma': 0.9136973898476178}\n",
            "\n",
            "Starting Optuna study for CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-08 23:29:43,391] Trial 0 finished with value: 0.46632229189142754 and parameters: {'iterations': 3704, 'learning_rate': 0.006576818256757044, 'depth': 6, 'l2_leaf_reg': 1.0344035214377897e-07, 'early_stopping_rounds': 162, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.35796284419040214}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:29:58,573] Trial 1 finished with value: 0.4676332624651785 and parameters: {'iterations': 1610, 'learning_rate': 0.03875450303928996, 'depth': 7, 'l2_leaf_reg': 4.353174559704348e-08, 'early_stopping_rounds': 199, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.15593046873363214}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:30:11,313] Trial 2 finished with value: 0.4669866597935444 and parameters: {'iterations': 2195, 'learning_rate': 0.018396326235043368, 'depth': 5, 'l2_leaf_reg': 0.01908886095782474, 'early_stopping_rounds': 73, 'bootstrap_type': 'MVS', 'subsample': 0.8784079719278406}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:31:11,638] Trial 3 finished with value: 0.474699801452436 and parameters: {'iterations': 2688, 'learning_rate': 0.034062974710765276, 'depth': 10, 'l2_leaf_reg': 2.5076706006826238e-05, 'early_stopping_rounds': 80, 'bootstrap_type': 'Bernoulli', 'subsample': 0.6654903198773288}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:31:34,316] Trial 4 finished with value: 0.4673681742367015 and parameters: {'iterations': 2235, 'learning_rate': 0.009885127731056806, 'depth': 6, 'l2_leaf_reg': 0.00649261831337343, 'early_stopping_rounds': 80, 'bootstrap_type': 'No'}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:33:37,294] Trial 5 finished with value: 0.4748288419926995 and parameters: {'iterations': 4928, 'learning_rate': 0.02894419892149978, 'depth': 10, 'l2_leaf_reg': 3.433610192878493e-05, 'early_stopping_rounds': 180, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.31482771170200374}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:38:07,320] Trial 6 finished with value: 0.4728585014565378 and parameters: {'iterations': 4368, 'learning_rate': 0.007986998990519285, 'depth': 10, 'l2_leaf_reg': 2.1019777876740203e-07, 'early_stopping_rounds': 180, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5154510706848128}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:38:59,871] Trial 7 finished with value: 0.47026689834598107 and parameters: {'iterations': 3022, 'learning_rate': 0.030642421993008682, 'depth': 9, 'l2_leaf_reg': 0.35503895613464903, 'early_stopping_rounds': 73, 'bootstrap_type': 'MVS', 'subsample': 0.6744429828852587}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:39:19,904] Trial 8 finished with value: 0.4713294872364428 and parameters: {'iterations': 4536, 'learning_rate': 0.049491043669807756, 'depth': 8, 'l2_leaf_reg': 0.0016701391308990233, 'early_stopping_rounds': 130, 'bootstrap_type': 'MVS', 'subsample': 0.7011391680985335}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:41:31,906] Trial 9 finished with value: 0.4729603280496022 and parameters: {'iterations': 1966, 'learning_rate': 0.018265191477008797, 'depth': 10, 'l2_leaf_reg': 2.6809410419679468e-08, 'early_stopping_rounds': 128, 'bootstrap_type': 'Bernoulli', 'subsample': 0.7686526220687613}. Best is trial 0 with value: 0.46632229189142754.\n",
            "[I 2025-06-08 23:41:48,707] Trial 10 finished with value: 0.4660149694404699 and parameters: {'iterations': 3523, 'learning_rate': 0.018835945195140362, 'depth': 5, 'l2_leaf_reg': 2.552700583194803e-06, 'early_stopping_rounds': 152, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.9015300689680378}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:42:04,690] Trial 11 finished with value: 0.4663854470532094 and parameters: {'iterations': 3665, 'learning_rate': 0.017971072002100418, 'depth': 5, 'l2_leaf_reg': 2.054302060045632e-06, 'early_stopping_rounds': 153, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.9288002961786532}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:43:04,805] Trial 12 finished with value: 0.4663496697088866 and parameters: {'iterations': 3705, 'learning_rate': 0.005571902785552904, 'depth': 6, 'l2_leaf_reg': 1.7379076165145607e-06, 'early_stopping_rounds': 153, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.7266323389792186}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:43:29,482] Trial 13 finished with value: 0.4661660689800634 and parameters: {'iterations': 3501, 'learning_rate': 0.01384660138232848, 'depth': 6, 'l2_leaf_reg': 1.6669943488690667e-06, 'early_stopping_rounds': 106, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.5409617911942415}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:43:48,872] Trial 14 finished with value: 0.4696600055427811 and parameters: {'iterations': 3167, 'learning_rate': 0.023149017175767523, 'depth': 7, 'l2_leaf_reg': 4.879908593125569e-05, 'early_stopping_rounds': 102, 'bootstrap_type': 'No'}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:44:08,316] Trial 15 finished with value: 0.46615043610597606 and parameters: {'iterations': 4068, 'learning_rate': 0.01423242593017556, 'depth': 5, 'l2_leaf_reg': 9.153659568321888e-07, 'early_stopping_rounds': 107, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.641756301525813}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:44:20,435] Trial 16 finished with value: 0.46609552200847515 and parameters: {'iterations': 4154, 'learning_rate': 0.02368899991037171, 'depth': 5, 'l2_leaf_reg': 0.00024808396695826895, 'early_stopping_rounds': 106, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.9770814090358966}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:44:41,246] Trial 17 finished with value: 0.4661597093567244 and parameters: {'iterations': 4917, 'learning_rate': 0.02466751431753556, 'depth': 5, 'l2_leaf_reg': 9.844867410130563, 'early_stopping_rounds': 139, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.978870336745737}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:44:59,706] Trial 18 finished with value: 0.4703305500227418 and parameters: {'iterations': 4274, 'learning_rate': 0.038069860010862475, 'depth': 8, 'l2_leaf_reg': 0.0003071175779497126, 'early_stopping_rounds': 96, 'bootstrap_type': 'No'}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:45:16,671] Trial 19 finished with value: 0.46840304602823385 and parameters: {'iterations': 2566, 'learning_rate': 0.023173587165693853, 'depth': 7, 'l2_leaf_reg': 0.00031674013006261197, 'early_stopping_rounds': 56, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.8059279280242703}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:45:27,208] Trial 20 finished with value: 0.46647114532139977 and parameters: {'iterations': 3358, 'learning_rate': 0.046747367937968465, 'depth': 5, 'l2_leaf_reg': 0.060308303397963656, 'early_stopping_rounds': 170, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.9945784790079778}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:45:46,416] Trial 21 finished with value: 0.4667773955682829 and parameters: {'iterations': 4065, 'learning_rate': 0.013600684834671247, 'depth': 5, 'l2_leaf_reg': 7.052319301837469e-06, 'early_stopping_rounds': 116, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.6723161474203724}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:46:05,536] Trial 22 finished with value: 0.4674666979321328 and parameters: {'iterations': 1056, 'learning_rate': 0.012835620564286623, 'depth': 6, 'l2_leaf_reg': 3.751713146992827e-07, 'early_stopping_rounds': 117, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.83451776537831}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:46:20,173] Trial 23 finished with value: 0.4663104231721064 and parameters: {'iterations': 4008, 'learning_rate': 0.021126098147089458, 'depth': 5, 'l2_leaf_reg': 0.0002693006566715013, 'early_stopping_rounds': 136, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.5723262564017082}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:46:33,399] Trial 24 finished with value: 0.4667135436263776 and parameters: {'iterations': 3957, 'learning_rate': 0.026977798466936032, 'depth': 6, 'l2_leaf_reg': 1.4201893028507972e-08, 'early_stopping_rounds': 91, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.8462503188824654}. Best is trial 10 with value: 0.4660149694404699.\n",
            "[I 2025-06-08 23:46:50,983] Trial 25 finished with value: 0.4659745477691998 and parameters: {'iterations': 4632, 'learning_rate': 0.015844658500528498, 'depth': 5, 'l2_leaf_reg': 9.03722417975122e-06, 'early_stopping_rounds': 117, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.6790116654180727}. Best is trial 25 with value: 0.4659745477691998.\n",
            "[I 2025-06-08 23:47:13,056] Trial 26 finished with value: 0.4657479671349753 and parameters: {'iterations': 4642, 'learning_rate': 0.0193518053947764, 'depth': 6, 'l2_leaf_reg': 1.0167183677294131e-05, 'early_stopping_rounds': 145, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5358416528696325}. Best is trial 26 with value: 0.4657479671349753.\n",
            "[I 2025-06-08 23:47:45,666] Trial 27 finished with value: 0.46698372113543396 and parameters: {'iterations': 4637, 'learning_rate': 0.017147996259875075, 'depth': 7, 'l2_leaf_reg': 0.00010447579358149298, 'early_stopping_rounds': 146, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5142966043505091}. Best is trial 26 with value: 0.4657479671349753.\n",
            "[I 2025-06-08 23:48:12,423] Trial 28 finished with value: 0.4659132317807213 and parameters: {'iterations': 4656, 'learning_rate': 0.010487599426744414, 'depth': 6, 'l2_leaf_reg': 8.887077003890723e-06, 'early_stopping_rounds': 119, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5785244107170697}. Best is trial 26 with value: 0.4657479671349753.\n",
            "[I 2025-06-08 23:48:42,707] Trial 29 finished with value: 0.4655779597048258 and parameters: {'iterations': 4658, 'learning_rate': 0.010548852408610771, 'depth': 6, 'l2_leaf_reg': 1.3391534905005663e-05, 'early_stopping_rounds': 119, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5854057096788899}. Best is trial 29 with value: 0.4655779597048258.\n",
            "[I 2025-06-08 23:49:17,265] Trial 30 finished with value: 0.46590071449429493 and parameters: {'iterations': 4930, 'learning_rate': 0.009947947831510547, 'depth': 6, 'l2_leaf_reg': 0.0014104969404690883, 'early_stopping_rounds': 140, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5805887565811785}. Best is trial 29 with value: 0.4655779597048258.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by Optuna for CatBoost: {'iterations': 4658, 'learning_rate': 0.010548852408610771, 'depth': 6, 'l2_leaf_reg': 1.3391534905005663e-05, 'early_stopping_rounds': 119, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5854057096788899}\n",
            "\n",
            "Training base models and collecting OOF and Test predictions...\n",
            "--- Fold 1 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 2 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 3 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 4 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 5 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 6 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 7 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 8 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 9 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "--- Fold 10 ---\n",
            "  Training LightGBM...\n",
            "  Training XGBoost...\n",
            "  Training CatBoost...\n",
            "\n",
            "LightGBM OOF RMSE (original scale): 20137812.37\n",
            "XGBoost OOF RMSE (original scale): 20244232.48\n",
            "CatBoost OOF RMSE (original scale): 20208137.85\n",
            "\n",
            "Weights for ensemble:\n",
            "  LightGBM: 0.3343\n",
            "  XGBoost:  0.3325\n",
            "  CatBoost: 0.3331\n",
            "\n",
            "Ensemble submission file created: submission_ensemble.csv\n",
            "\n",
            "Ensemble OOF RMSE (original scale): 20165399.37\n",
            "\n",
            "Further tuning, feature engineering, or different models might be needed. Current Ensemble OOF RMSE: 20165399.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bFue5_NXzT6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}